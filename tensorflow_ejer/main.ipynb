{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbca9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tkinter import Tk, filedialog\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88fe09",
   "metadata": {},
   "source": [
    "***PROCESAMIENTO DE DATOS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe748e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17526 files belonging to 2 classes.\n",
      "Found 2290 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data/CatvsDog\"  # <-- CAMBIA ESTO a tu ruta local\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (150, 150)\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "val_dir = os.path.join(DATA_DIR, 'validation')\n",
    "\n",
    "ds_train = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "ds_val = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c98c5",
   "metadata": {},
   "source": [
    "***NORMALIZAR IMAGENES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5d5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "ds_train = ds_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "ds_val = ds_val.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Prefetch\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "ds_val = ds_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5704ad",
   "metadata": {},
   "source": [
    "***MODELO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7b0cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Matias\\py\\mi_entorno\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = models.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "model = models.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56369c77",
   "metadata": {},
   "source": [
    "***ENTRENAMIENTO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc8024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 399ms/step - accuracy: 0.6590 - loss: 0.6605 - val_accuracy: 0.5611 - val_loss: 0.8289\n",
      "Epoch 2/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 209ms/step - accuracy: 0.7224 - loss: 0.5768 - val_accuracy: 0.5323 - val_loss: 1.0394\n",
      "Epoch 3/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 195ms/step - accuracy: 0.7211 - loss: 0.5797 - val_accuracy: 0.5323 - val_loss: 1.0609\n",
      "Epoch 4/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 230ms/step - accuracy: 0.7210 - loss: 0.5828 - val_accuracy: 0.4699 - val_loss: 1.8699\n",
      "Epoch 5/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 247ms/step - accuracy: 0.7169 - loss: 0.5859 - val_accuracy: 0.5764 - val_loss: 0.7636\n",
      "Epoch 6/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 336ms/step - accuracy: 0.7266 - loss: 0.5650 - val_accuracy: 0.5323 - val_loss: 0.7176\n",
      "Epoch 7/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 194ms/step - accuracy: 0.7149 - loss: 0.5968 - val_accuracy: 0.5310 - val_loss: 0.8791\n",
      "Epoch 8/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 148ms/step - accuracy: 0.7147 - loss: 0.5970 - val_accuracy: 0.5314 - val_loss: 0.7205\n",
      "Epoch 9/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 171ms/step - accuracy: 0.7132 - loss: 0.5941 - val_accuracy: 0.5419 - val_loss: 1.0861\n",
      "Epoch 10/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 186ms/step - accuracy: 0.7233 - loss: 0.5830 - val_accuracy: 0.5415 - val_loss: 0.7166\n",
      "Epoch 11/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 149ms/step - accuracy: 0.7154 - loss: 0.5941 - val_accuracy: 0.5681 - val_loss: 0.7200\n",
      "Epoch 12/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 155ms/step - accuracy: 0.7269 - loss: 0.5793 - val_accuracy: 0.5773 - val_loss: 0.7079\n",
      "Epoch 13/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 156ms/step - accuracy: 0.7182 - loss: 0.5923 - val_accuracy: 0.5328 - val_loss: 0.7221\n",
      "Epoch 14/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 177ms/step - accuracy: 0.7154 - loss: 0.5920 - val_accuracy: 0.5581 - val_loss: 1.2263\n",
      "Epoch 15/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 195ms/step - accuracy: 0.7271 - loss: 0.5785 - val_accuracy: 0.5817 - val_loss: 0.6891\n",
      "Epoch 16/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 218ms/step - accuracy: 0.7306 - loss: 0.5733 - val_accuracy: 0.5821 - val_loss: 0.7052\n",
      "Epoch 17/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 379ms/step - accuracy: 0.7249 - loss: 0.5768 - val_accuracy: 0.5747 - val_loss: 3.7178\n",
      "Epoch 18/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 169ms/step - accuracy: 0.7363 - loss: 0.5663 - val_accuracy: 0.5690 - val_loss: 0.6898\n",
      "Epoch 19/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 149ms/step - accuracy: 0.7289 - loss: 0.5782 - val_accuracy: 0.5952 - val_loss: 0.7000\n",
      "Epoch 20/20\n",
      "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 153ms/step - accuracy: 0.7319 - loss: 0.5670 - val_accuracy: 0.5371 - val_loss: 0.8310\n"
     ]
    }
   ],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=20,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb60b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('cats_vs_dogs_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086b5b0",
   "metadata": {},
   "source": [
    "***EVALUACION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9edc33b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mds_val\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_val' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, accuracy = model.evaluate(ds_val)\n",
    "print(f'Validation Loss: {loss:.4f}')\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in ds_val:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.round(preds).flatten())\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=['Cat', 'Dog']))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dcfb6",
   "metadata": {},
   "source": [
    "***PREDICCION IMAGEN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cb97c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar dataset de validación\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m (ds_train, ds_val), ds_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcats_vs_dogs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:]\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Preprocesamiento\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(img, label):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nSelecciona una imagen para predecir...\")\n",
    "root = Tk()\n",
    "root.withdraw()\n",
    "img_path = filedialog.askopenfilename(\n",
    "    title='Seleccionar imagen',\n",
    "    filetypes=[('Image Files', '*.jpg *.jpeg *.png')]\n",
    ")\n",
    "\n",
    "if img_path:\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        print(f'La imagen seleccionada es un Gato')\n",
    "    else:\n",
    "        print(f'La imagen seleccionada es un Perro')\n",
    "\n",
    "    img = image.load_img(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Gato\" if prediction < 0.5 else \"Perro\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se seleccionó ninguna imagen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
